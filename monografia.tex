% Numeração de acordo com UFPR
\documentclass[pnumromarab,normaltoc]{abnt}	

\usepackage[brazil]{babel}
\usepackage[latin1]{inputenc}
\usepackage{abnt-alf}
\usepackage{graphicx}
\usepackage{url}
\usepackage{remreset}

\makeatletter
\@removefromreset{footnote}{chapter}
\makeatother

\makeatletter	% Para que ele entenda o @

% CAPA
%\renewcommand{\capa} {
%\begin{titlepage}
%	\espaco{1.1}
%	
%	\begin{center}
%		\large\ABNTchapterfont\ABNTautordata
%	\end{center}
%	
%	\vspace{7.5cm}
%	
%	\begin{center}
%		\large\ABNTchapterfont\ABNTtitulodata\par
%	\end{center}
%	
%	\vfill
%	
%	\begin{center}
%		\textbf{\ABNTlocaldata}\par
%		\textbf{\ABNTdatadata}
%	\end{center}
%\end{titlepage}
%}

% FOLHA DE ROSTO
\newcommand{\esporient}[2] {
	\leftskip 0em
	\@tempdima 5.5em
	\advance\leftskip \@tempdima \null\nobreak\hskip -\leftskip
	{#1#2\hfil}
}

\newcommand{\espcoorient}[2] {
	\leftskip 0em
	\@tempdima 7em
	\advance\leftskip \@tempdima \null\nobreak\hskip -\leftskip
	{#1#2\hfil}
}

\renewcommand{\folhaderosto} {
\begin{titlepage}
	\espaco{1.1}
	
	\begin{center}
		\large\ABNTchapterfont\ABNTautordata
	\end{center}
	
	\vspace{7.5cm}
	
	\begin{center}
		\large\ABNTchapterfont\ABNTtitulodata\par
	\end{center}
	
	\vspace{2cm}
	
	\hspace{.35\textwidth}
	\begin{minipage}{.5\textwidth}
		\begin{espacosimples}
			\ABNTcomentariodata\par
		\end{espacosimples}
	\end{minipage}
	
	\hspace{.35\textwidth}
	\begin{minipage}{.5\textwidth}
		\begin{espacosimples}
			\esporient{\numberline {Orientador:}}{\ignorespaces\ABNTorientadordata}
		\end{espacosimples}
	\end{minipage}
	
	\ABNTifnotempty{\ABNTcoorientadordata}{
		\hspace{.35\textwidth}
		\begin{minipage}{.5\textwidth}
			\begin{espacosimples}
				\espcoorient{\numberline {Co-Orientador:}}{\ignorespaces\ABNTcoorientadordata}
			\end{espacosimples}
		\end{minipage}}
	
	\vfill
	
	\begin{center}
		\textbf{\ABNTlocaldata}\par
		\textbf{\ABNTdatadata}
	\end{center}

\end{titlepage}
}

% Altera o tamanho das fontes dos capítulos e dos apêndices
\renewcommand{\ABNTchapterfont}{\bfseries}
\renewcommand{\ABNTchaptersize}{\Large}
\renewcommand{\ABNTanapsize}{\Large}

% Altera o espaçamento entre dots
\renewcommand\@dotsep{2}

% Altera forma de montagem do TOC
\renewcommand\l@chapter[2]{
  \ifnum \c@tocdepth >\m@ne
    \addpenalty{-\@highpenalty}%
    \vskip 1.0em \@plus\p@
    \setlength\@tempdima{1.5em}%
    \begingroup
      \ifthenelse{\boolean{ABNTpagenumstyle}}
        {\renewcommand{\@pnumwidth}{3.5em}}
        {}
      \parindent \z@ \rightskip \@pnumwidth
      \parfillskip -\@pnumwidth
      \leavevmode \normalsize\ABNTtocchapterfont
      \advance\leftskip\@tempdima
      \hskip -\leftskip
      #1\nobreak\dotfill \nobreak%
      \ifthenelse{\boolean{ABNTpagenumstyle}}
         {%
          \hb@xt@\@pnumwidth{\hss 
            \ifthenelse{\not\equal{#2}{}}{{\normalfont p.\thinspace#2}}{}}\par
         }
         {%
          \hb@xt@\@pnumwidth{\hss #2}\par
         }
      \penalty\@highpenalty
    \endgroup
  \fi
}

\renewcommand*\l@section{\@dottedtocline{1}{0em}{2.3em}}
\renewcommand*\l@subsection{\@dottedtocline{2}{0em}{3.2em}}
\renewcommand*\l@subsubsection{\@dottedtocline{3}{0em}{4.1em}}

% Cria um comando auxiliar para montagem da lista de figuras
%\newcommand{\figfillnum}[1]{%
%  {\hspace{1em}\normalfont\dotfill}\nobreak
%  \hb@xt@\@pnumwidth{\hfil\normalfont #1}{}\par}

% Cria um comando auxiliar para montagem da lista de tabelas
%\newcommand{\tabfillnum}[1]{%
%	{\hspace{1em}\normalfont\dotfill}\nobreak
%	\hb@xt@\@pnumwidth{\hfil\normalfont #1}{}\par}

% Altera a forma de montagem da lista de figuras
\renewcommand*{\l@figure}[2]{
	\leftskip 3.1em
	\rightskip 1.6em
	\parfillskip -\rightskip
	\parindent 0em
	\@tempdima 2.0em
	\advance\leftskip \@tempdima \null\nobreak\hskip -\leftskip
	{Figura \normalfont #1}\nobreak \figfillnum{#2}}

% Altera a forma de montagem de lista de tabelas
\renewcommand*{\l@table}[2]{
	\leftskip 3.4em
	\rightskip 1.6em
	\parfillskip -\rightskip
	\parindent 0em
	\@tempdima 2.0em
	\advance\leftskip \@tempdima \null\nobreak\hskip -\leftskip
	{Tabela \normalfont #1}\nobreak \tabfillnum{#2}}

% Define os comandos que montam a lista de símbolos
\newcommand{\listadesimbolos}{\pretextualchapter{Lista de Símbolos}\@starttoc{lsb}}
\newcommand{\simbolo}[2]{{\addcontentsline{lsb}{simbolo}{\numberline{#1}{#2}}}#1}
\newcommand{\l@simbolo}[2]{
	\vspace{-0.75cm}
	\leftskip 0em
	\parindent 0em
	\@tempdima 5em
	\advance\leftskip \@tempdima \null\nobreak\hskip -\leftskip
	{\normalfont #1}\hfil\nobreak\par}

% Define o comando que monta a lista de siglas
\newcommand{\listadesiglas}{\pretextualchapter{Lista de Siglas}\@starttoc{lsg}}
\newcommand{\sigla}[2]{{\addcontentsline{lsg}{sigla}{\numberline{#1}{#2}}}#1}
\newcommand{\l@sigla}[2]{
	\vspace{-0.75cm}
	\leftskip 0em
	\parindent 0em
	\@tempdima 5em
	\advance\leftskip \@tempdima \null\nobreak\hskip -\leftskip
	{\normalfont #1}\hfil\nobreak\par}

% Define o tipo de numeração das páginas
\renewcommand{\chaptertitlepagestyle}{plain}

% Altera a posição da numeração de páginas dos elementos pré-textuais
\renewcommand\pretextualchapter{
	\if@openright\cleardoublepage\else\clearpage\fi
	\pagestyle{\chaptertitlepagestyle}
	\global\@topnum\z@
	\@afterindentfalse
	\@schapter}

% Altera a posição da numeração de páginas dos elementos textuais
\renewcommand{\ABNTchaptermark}[1]{
	\ifthenelse{\boolean{ABNTNextOutOfTOC}}
		{\markboth{\ABNTnextmark}{\ABNTnextmark}}
		{\chaptermark{#1}
		\pagestyle{\chaptertitlepagestyle}}}

% Redefine o tipo de numeração das páginas
\renewcommand{\ABNTBeginOfTextualPart}{
	\renewcommand{\chaptertitlepagestyle}{plainheader}
	\renewcommand{\thepage}{\arabic{page}}
	\setcounter{page}{1}}

\makeatother

% Altera o tamanho do parágrafo
\setlength{\parindent}{1.5cm}

% DOCUMENTO
\begin{document}

\autor{CAIO ESDRAS DE BRITO BEGOTTI}
\titulo{MÉTODO PARA CRIAÇÃO E ANÁLISE DE CORPORA LINGUÍSTICOS EM LATIM CLÁSSICO UTILIZANDO NLTK}
\orientador{Alessandro Rolim de Moura}

% Se tiver co-orientador
%\coorientador{Co-orientador 1\protect\\Co-orientador 2}

\comentario{Projeto de Monografia apresentado à disciplina Orientação Monográfica I do Curso de Letras (Bacharelado em Estudos Linguísticos), do Setor de Ciências Humanas, Letras e Artes da Universidade Federal do Paraná.}

\local{CURITIBA}

\data{DEZEMBRO DE 2011}

%\capa
\folhaderosto

% TERMO DE APROVAÇÃO
%\begin{titlepage}
%	\espaco{1.1}
%	
%	\begin{center}
%		\Large\textbf{{Termo de Aprovação}}
%	\end{center}
%	
%	\vspace{0.75cm}
%	
%	\begin{center}
%		\large\ABNTautordata
%	\end{center}
%	
%	\vspace{1cm}
%	
%	\begin{center}
%		\large\ABNTtitulodata
%	\end{center}
%	
%	\vspace{1cm}
%	
%	\noindent Dissertação aprovada como requisito parcial para obtenção do grau de Mestre/Doutor em Minha Área, pelo Programa de Pós-Graduação, Setor, Universidade Federal do Paraná, pela seguinte banca examinadora:
%	
%	\setlength{\ABNTsignthickness}{0.4pt}
%	\setlength{\ABNTsignskip}{2cm}
%	
%	\vspace{-0.5cm}
%	\assinatura{Prof. Dr. Meu Orientador\\Universidade Federal do Paraná}
%	
%	\vspace{-0.5cm}
%	\assinatura{Prof. Dr. Meu Co-orientador\\Universidade Federal do Paraná}
%	
%	\vspace{-0.5cm}
%	\assinatura{Prof. Dr. Convidado\\Universidade} 
%	
%	\vspace{-0.5cm}
%	\assinatura{Prof. Dr. Convidado \\Universidade}
%	
%	\vfill
%	
%	\begin{center}
%		Curitiba, XX de ZZ de YYYY
%	\end{center}
%
%\end{titlepage}

% Deve ser adicionado ao contador de páginas um, referente a folha de rosto.
% A folha de aprovação não recebe número, nem é contada.
\addtocounter{page}{1}

%\pretextualchapter{Dedicatória}
%
%\vspace{12cm}
%\hspace{.3\textwidth}
%\begin{minipage}{.6\textwidth}
%	\par A quem eu dedico,
%	\par $\phantom{linha em branco}$
%	\par Por muito que fizeram, eu fiz tudo, só agradeço por educação...
%\end{minipage}

\newpage

%\pretextualchapter{Agradecimentos}
%
%\vspace{12cm}
%\hspace{.3\textwidth}
%\begin{minipage}{.6\textwidth}
%	\par A todos que, direta ou indiretamente, contribuíram para a realização e divulgação deste trabalho.
%\end{minipage}

%\pretextualchapter{Epígrafe}
%
%\vspace*{12cm}
%\hspace{.3\textwidth}
%\begin{minipage}{.6\textwidth}
%	\par Uma pequena prosa que esteja relacionada com o trabalho.
%	\par Obs.: A forma como uma prosa é escrita é importante.
%\end{minipage}

\sumario

% 1 - Lista de Figuras
%\listadefiguras

% 2 - Lista de Tabelas
%\listadetabelas

% 3 - Lista de Siglas, e.g. \sigla{sigla}{Descrição}
\listadesiglas

% 4 - Lista de Símbolos, e.g. \simbolo{símbolo}{Descrição}
%\listadesimbolos

% RESUMO
%\begin{resumo}
%	$\phantom{linha em branco}$\\
%	Escreva aqui o texto de seu resumo...\\
%	$\phantom{linha em branco}$\\
%	Palavras-chave: Escreva; Aqui; Suas; Palavras-chave.
%\end{resumo}

% ABSTRACT
%\begin{abstract}
%	$\phantom{linha em branco}$\\
%	Write here the English version of your `Resumo'...\\
%	$\phantom{linha em branco}$\\
%	Key-words: Write; Here; Your; Key-words.
%\end{abstract}

% A parte textual deve conter:
% 1 - Introdução
% 2 - Revisão de Literatura
% 3 - Material e Métodos
% 4 - Análise dos Resultados
% 5 - Discussão
% 6 - Conclusão

\chapter{Introdução}
\par O projeto de monografia que aqui se apresenta foca na construção de corpora linguísticos de textos do período clássico do latim e em uma análise computacional desses corpora. O propósito por trás dessas duas tarefas é o de sugerir melhoras em métodos de ensino do latim e permitir uma melhor compreensão dessa língua de forma mais empírica, ou seja, uma compreensão baseada no uso que um dia foi dado a língua através da análise de textos reais em latim clássico.

\par Para tal, esse trabalho se baseará no projeto \sigla{NLTK}{Natural Language Toolkit} (Natural Language Toolkit), que consiste em um conjunto de rotinas de programação de computador, em forma de kit de ferramentas\footnote{A descrição pode soar estranha em português e para não iniciados em programação; em bibliografias de computação ambos os termos aparecem simplesmente como \sigla{API}{Application Programming Interface} e toolkit, em inglês}, para processamento de linguagem natural. Com a ajuda do NLTK e à luz da Linguística de Corpus (\sigla{LC}{Linguística de Corpus}) procurarei evidenciar os usos mais comuns do latim clássico por Cícero e irei sugerir uma abordagem para uso disso em sala de aula.

\par Marcus Tullius Cicero, daqui para frente simplesmente Cícero, será o autor escolhido para a montagem dos corpora. Cícero é um dos autores que mais representam o período clássico do latim --- tanto pela grande produção e variedade de estilos de seus textos quanto pelos seus discursos ---, e devido a sua importância após a Renascença nada mais natural do que usá-lo nas análises desse trabalho.

\par Espera-se ainda, com esse trabalho, contribuir para o ainda pequeno grupo de linguistas computacionais no Brasil, especialmente os que apreciam e trabalham com o latim. A exemplo do que já foi feito com o inglês utilizando métodos semelhantes ao que aqui serão apresentados, pode-se ganhar bastante em sala de aula e pesquisa com tal abordagem: aproximando a linguística de corpus à linguística computacional e línguas naturais \cite{sardinha-palavras}.

\chapter{Justificativas}
\par A seguir se apresentam os problemas e as propostas que justificam este trabalho.

\section{Problemas a serem solucionados}
\par É ponto pacífico que atualmente o ensino de latim se dá de forma bastante limitada em escolas e universidades brasileiras. Todavia, o latim clássico parece estar em uma espécie de retomada em países como Inglaterra (que  vale dizer sempre foi um centro de referência em estudos clássicos) e Estados Unidos. Nos Estados Unidos, por exemplo, relatórios mostram o latim entre as oito línguas mais estudadas hoje no país, na frente do russo e português com crescimento de mais de 20\% em matrículas nos últimos anos \cite{mla}. De qualquer maneira, parte dessas matrículas ainda é para cursos introdutórios onde alunos pouco conhecem sobre a língua e dependem exclusivamente dos professores.

\par Muitas vezes o latim acaba sendo ensinado somente de cima para baixo, ou seja, dos professores aos alunos, pois os primeiros são uma fonte natural de conhecimento e autoridade. Assim, alunos de latim com frequência simplesmente tem que confiar na capacidade de julgamento dos mestres na escolha do melhor material a ser estudado. O que de fato faz sentido, pois os professores sabem avaliar a melhor linha de aprendizado. Mas embora os professores possam fazer boas escolhas quanto ao textos e ao vocabulário a serem estudados, tais escolhas serão sempre parciais ou limitadas, de modo que a criação de materiais e métodos que fundamentem tais escolhas de forma objetiva é sempre bem-vinda.

\par Por si só isso não representa um grande problema. Entretanto, por se tratar de uma língua sem usuários nativos --- logo, poucos podem julgar se o que está sendo estudado de fato representaria a realidade dos usuários de uma língua de dois mil anos atrás --- ninguém na realidade sabe dizer com total certeza se, por exemplo, a seleção de textos e a escolha de determinado vocabulário de estudo são ou não adequados, ou ainda se ele é suficientemente representativo para os alunos ganharem proficiência mais rapidamente.

\par Isso é até aceitável em línguas estrangeiras modernas, afinal o aluno tem mais contato com elas no seu dia-a-dia e pode sair de um beco sem saída com mais facilidade e sem recorrer à ajuda dos professores, mas com línguas clássicas como o latim, e o grego, o prejuízo pode se tornar maior para o aluno que se dedicou por tanto tempo e então percebe que tomou um caminho de estudo não muito bom e perdeu uma melhor oportunidade de aprendizado. É claro que espera-se dos alunos que eles possam compensar eventuais lacunas desse aprendizado através de estudo e leituras próprias, mas infelizmente não se pode contar com isso sempre.

\par Em uma situação limite, um estudante de latim clássico poderia estar aprendendo um vocabulário que outrora pertenceu a um texto menor e que não corresponde a realidade linguística que ele buscava, ou que também não facilita a assimilação de idéias do estilo que procurava entender. Naturalmente, deve-se considerar aqui que o ensino de latim costumava focar aspectos altamente literários e clássicos. Por exemplo, alunos de latim comumente lêem Cícero após um ou dois anos de estudo, mas Cícero somente representa uma fatia pequena do todo que foi o latim em sua época. Deixa-se então o latim vulgar, falado pelo povo, para aulas de linguística românica e textos religiosos para aulas sobre latim medieval ou eclesiástico.

\section{Diferenciais tecnológicos}
\par Parece que um dos motivos pelo qual isso ocorre é a falta de um catálogo linguístico cientificamente montado com as palavras de fato mais usadas da língua. Já houve tentativas de se fazer isso para o latim, porém em uma época cujos poderes da análise linguística feita pelo computador ainda não eram conhecidos \cite{diederich}, ou foram tentativas focadas em outras línguas, como o inglês \cite{almeida-dicasl} e o espanhol \cite{jacobi}. Outros autores, como \citeonline{pellegrino}, no máximo se limitaram a criar listas de palavras bastante usadas por Cícero a serem estudadas para exames de latim de escolas americanas, sem uma maior racionalização sobre os corpora de Cícero e sem também elaborar em seus métodos. Um trabalho bastante superficial e parecendo-se mais como manual de prova. Embora os resultados de todas essas experiências tenham sido utilizados de maneira distinta ao que aqui se propõe, a motivação é similar: utilizar artefatos reais da língua para o aprendizado dela mesma.

\par Criando-se hoje corpora linguísticos do latim em um formato reconhecido por linguistas computacionais, utilizando-se métodos documentados e comprovadamente eficientes para análise linguística, é possível melhorarmos o ensino de latim em escolas e universidades, além de permitir ao aluno --- independente ou aconselhado --- que tome o ensino da língua nas suas próprias mãos. Como bem diz \citeonline{jacobi}, \textit{essa forma de trabalho enfatiza o desenvolvimento da habilidade de descoberta nos alunos. A aprendizagem movida a dados posiciona o aluno no papel de descobridor ou de pesquisador, e o professor passa a ter como função primordial propiciar meios para que os alunos desenvolvam estratégias de descoberta}. Para tal, o volume de dados a ser analisado e utilizado nem mesmo precisa ser muito grande para se notar bons ganhos no aprendizado \cite{willis}.

\par Ainda que existam diversos modelos, bases de dados e programas de computador para a análise de corpora, muitos possuem seus códigos fechados ou requerem licenciamentos, ou seja, não permitem o uso irrestrito deles por parte dos usuários e pesquisadores caso estes desejem, por exemplo, aplicar melhorias nesses softwares ou corrigir falhas nas análises por conta própria. Além disso, alguns são bastante caros --- o que porém é compreensível dada a dificuldade de catalogar e organizar bases enormes ---, e fogem da realidade acadêmica brasileira, ainda carente de recursos para latinistas\footnote{Softwares para análise linguística, por mais simples que sejam suas funcionalidades, não custam menos que algumas dezenas de dólares, quando não centenas. Bancos de dados prontos com corpora costumam também cobrar licenças de uso temporário que podem em alguns casos chegar a milhares de euros ou, embora ``gratuitas'', acabam restringindo o uso e pesquisa comercial através deles.}. Logo, uma solução aberta, sem nenhum tipo de restrição e preferivelmente barata se faz necessária. Essa solução, que será detalhada nas próximas seções, deve ainda aproveitar a possibilidade de utilizar textos sem amarras de copyright como no caso de textos clássicos em latim. Portanto, não está sendo proposta aqui uma revolução no jeito de trabalhar o latim com computação, mas simplesmente uma alternativa viável, barata e eficiente para alunos, pesquisadores e entusiastas brasileiros.

\section{Melhorias no ensino}
\par A busca pelo domínio da leitura em uma segunda língua é um dos maiores objetivos de qualquer interessado em línguas, e isso não é diferente para o latim. Esse trabalho se propõe a ajudar nisso. A motivação inicial então veio através de um projeto similar focado na língua inglesa na UNICAMP. Hoje transformado em livro e guia de estudo, o projeto começou como um mero catálogo das palavras mais comuns do inglês, e desencadeou novas formas de estudar a língua que poderiam beneficiar também o latim \cite{almeida-livro}.

A força que instiga esse trabalho a responder essas questões é muito bem explicada por \citeonline{sapir}, que já acreditava que \textit{vocabulary is a very sensitive index of the culture of a people and changes of meaning, loss of old words, the creation and borrowing of new ones are all dependent on the history of culture itself. Languages differ widely in the nature of their vocabularies. Distinctions which seem inevitable to us may be utterly ignored in languages which reflect an entirely different type of culture, while these in turn insist on distinctions which are all but unintelligible to us. Such differences of vocabulary go far beyond the names of cultural objects, such as arrow point, coat of armor or gunboat. They apply just as well to the mental world}.

\par Enfim, tal abordagem se mostra além de eficiente, bastante realista, ao passo que aponta para os alunos somente o que vale a pena ser aprendido e aprendido não através de contextos artificiais ou idealizados da  língua.

\chapter{Fundamentação Teórica}

\section{Linguística Aplicada e Computacional}
\par Todo esse trabalho se sustenta sobre os princípios da Linguística Aplicada (\sigla{LA}{Linguística Aplicada}). A LA por sua vez remete ao uso de ferramentas linguísticas destinado à resolução e análise de problemas reais das línguas, ou seja, é o estudo prático e não teórico delas. Particularmente, o subcampo da LA mais em voga atualmente é o da linguística computacional, que de forma simplificada é um campo de estudo que envolve diversas áreas como: ciência da computação, matemática, neurologia e a própria linguística, para citar os mais populares.

\par A história da linguística computacional está, naturalmente, ligada ao surgimento dos primeiros computadores logo após a II Guerra Mundial. Muitos problemas linguísticos hoje envolvem resolver algorítimos ou modelar dados --- com frequência em grande escala --- e foi exatamente para estes tipos de tarefas que os computadores foram criados. Dessa forma, não é surpresa alguma que ambas as áreas estejam hoje tão próximas; no mundo privado ao menos, pois na academia brasileira tenho a impressão que ainda não alcançamos massa crítica de pesquisa ainda \cite{ladeira}.

\par Do contato da linguística e da computação, então, chegamos ao Processamento de Linguagem Natural (\sigla{PLN}{Processamento de Linguagem Natural}) --- ou \sigla{NLP}{Natural Language Processing} de Natural Language Processing, como costuma aparecer em bibliografias em inglês ---, que basicamente foca em problemas como análise automática de discurso, tradução por máquina, análises morfo-sintáticas, reconhecimento ou geração de fala\footnote{Speech-to-text e text-to-speech}, etiquetamento de anunciados\footnote{Tagging}, lematização de palavras\footnote{Word stemming}, gramática categorial, prosódia semântica de textos\footnote{Sentiment analysis}, análises estatísticas, entre  outros. Pelas duas áreas de pesquisa andarem quase sempre juntas atualmente, poucos conseguem distinguir diferenças entre elas. A cada dia surgem novas aplicações possíveis além das listadas e convencionou-se chamar tudo isso simplesmente de processamento de linguagem natural.

\section{Linguística de Corpus}
\subsection{Histórico}
\par Dentro da linguística computacional encontramos afinal a LC, campo da linguística que estuda a língua como objeto vivo através de modelos estatísticos e amostragens de dados reais da língua, não representações idealizadas dela. É um campo da linguística totalmente empirista.

\par Para os aderentes a essa linha teórica, a língua deve se expressar por si mesma, pelos textos e discursos que surgem naturalmente de seus usuários, e daí o linguista pode aferir fatos e analisá-la. A linguística de corpus, dessa maneira, evita trabalhar com textos criados artificialmente, e seus modelos são modelos de dados e não modelos teóricos de representação da língua; podem ser textos escritos ou falas transcritas, organizados em um dado formato e com categorização padronizada, além de poderem ser manualmente mapeados para um processamento posterior.

\par Antigamente a Linguística de Corpus se encontrava em um estado bastante primitivo, muitas vezes sendo trabalhada por formas manuais. Hoje em dia ela é um campo de estudo que anda junto com a linguística computacional, dado o poder de processamento e automatização de um computador a serviço do linguista \cite{sinclair}, comparado a um linguista solitário com papel e lápis.

\subsection{Casos de uso}
\par Somente nas últimas décadas a linguística de corpus tomou o palco das ciências das línguas. Por exemplo, foi somente por volta dos anos 70 que os primeiros corpora de línguas faladas foram montados e analisados digitalmente\footnote{Os dicionários Collins COBUILD English Language Dictionary e The American Heritage Dictionary of the English Language foram os primeiros a utilizar a LC em sua construção}. Portanto, não é de surpreender que tão pouco a utilizem ainda e, particularmente no Brasil, exista certa falta de tradição nesse ramo da linguística aplicada.

\par Hoje, virtualmente todas as ferramentas digitais de tradução, análise linguística e construção de dicionários utilizam a LC como base. Grandes empresas, como Google e IBM, utilizam de seu poder computacional e infra-estruturas gigantescas para, através de trilhões de fontes de textos disponíveis na Internet --- um amontoado de corpora de domínio público esperando para ser analisado ---, construir conversores de texto para fala, analisadores sintáticos, tradutores automáticos etc.

\par No caso dos tradutores automáticos, por exemplo, é graças à linguística de corpus que recentemente o serviço Google Translate passou a suportar a tradução de textos em diversas línguas do e para o latim, já que seria quase impossível implementar de forma fixa todas as regras gramaticais do latim em um tradutor automático. Embora os resultados de ferramentas como o Google Translate ainda não pareçam muito profissionais, o fato é que traduções baseadas em estatísticas de corpora tendem a apresentar melhores resultados que as baseadas em regras linguísticas fixas \cite{och}. A qualidade dos resultados está diretamente relacionada ao volume de dados a serem processados. É por isso que muitas vezes textos de serviços como o do Google ainda mostram ter uma sintaxe estranha. Quanto maiores e mais abrangentes forem os corpora, melhores serão as traduções\footnote{Uma das formas mais populares para avaliação de traduções automáticas baseadas em estatísticas de corpora é a pontuação \sigla{BLEU}{Bilingual Evaluation Understudy}, proposta pela IBM, que indica o grau de inteligibilidade de um texto traduzido por um computador comparando-o a um traduzido por um ser humano, atestando sua qualidade}

\chapter{Objetivos}
\par O objetivo primordial desse trabalho é permitir uma maior compreensão de textos clássicos em latim para os estudantes dessa língua através da criação de vocabulários específicos e automatizados por processamento de linguagem natural, cujas fontes serão textos de Cícero, do período clássico do latim. De forma análoga a \citeonline{almeida-livro}, esperamos que alunos de latim consigam se beneficiar do mesmo modelo de aprendizado, comprovado para o inglês. Dessa maneira, poderemos encontrar respostas, que não são facilmente comprováveis sem métodos como os propostos nesse trabalho, para perguntas como:

\begin{enumerate} \item que vocabulário é mais adequado para determinados alunos ou pesquisadores em um contato inicial com o latim clássico? \item entre os textos clássicos em latim deixados por Cícero, que características lexicais eles apresentam? \item que tipo de palavreado era mais comum em textos oficiais, e qual era mais comum em textos informais (como em cartas)?\end{enumerate}
 
\par Através desses resultados formatados, alunos e professores poderão focar no aprendizado dos textos em si, não desperdiçando tempo em decorar vocabulários sem necessidade, assim como já foi feito para outras línguas.

\par Indiretamente, teremos um objetivo secundário que deriva do propósito anterior: a criação de corpora e um catálogo de stopwords do latim. Os corpora poderão ser utilizados para processamento de linguagem natural por terceiros, da mesma forma que serão utilizados nesse trabalho. O catálogo de stopwords poderá também ser utilizado por terceiros, e tentará ser abrangente, pois atualmente não se tem um catálogo completo para o latim assim como se tem para outras línguas.

\chapter{Metodologia}

\section{NLTK}
\par 	Nesse trabalho será utilizado o NLTK um conjunto de ferramentas de programação voltado ao processamento de linguagem natural através de computação. Atualmente o NLTK é um dos projetos de maior sucesso para esse fim, pois, além de ser livre de licenças e custos, é bastante abrangente, cobre diversas línguas oficialmente e é disponível já com diversos corpora para testes. O projeto possui livros e publicações sobre seu funcionamento, tem mais de dez anos de desenvolvimento e é utilizado em universidades há algum tempo, tornando-o ideal para uso uma vez que se provou estar em constante evolução acompanhando novas descobertas no campo de processamento de linguagem natural \cite{bird}.

\section{Corpora}

\subsection{Especificação}

\par Os corpora de Cícero serão construídos de acordo com as regras de formatação e organização de outros corpora encontrados no projeto NLTK, que atualmente possui cerca de quarenta corpora prontos para uso. Isso facilitará bastante o processamento dos textos e também contribuirá com o projeto, para aumentar --- criar, na realidade --- sua base de textos em latim. Tentaremos também, se o tempo permitir, iniciar o etiquetamento gramatical dos corpora, mesmo que de forma rudimentar.

\par O escopo dos textos escolhido para compor os corpora desse trabalho abrange o período clássico da literatura latina, da qual Cícero é um dos maiores expoentes, senão o maior. A escolha de Cícero se deu por sua produção ter sido bastante variada, tendo ele escrito desde tratados até cartas pessoais, de textos jurídicos até filosofia. Cícero teve uma grande influência no pensamento moderno, e é ainda um dos autores mais estudados em cursos de latim, o que o torna ideal para uma análise linguística do latim.

\par Quanto ao tamanho dos corpora, que indiretamente determina a utilidade deles, em um teste preliminar chegou-se a marca de 80.000 palavras. No teste, apenas alguns textos de Cícero facilmente encontráveis publicamente foram processados, sem muito método. De acordo com estimativas de \citeonline{sardinha-wordsmith}, isso já garante a esse trabalho corpora de tamanhos razoáveis. O tamanho final dos corpora deverá ser muito maior que esse número, porém.

\subsection{Por que Cícero?}

\par A influência e importância de Cícero como pensador jamais serão suficientemente demonstradas, porém é evidente que o latim como língua deve muito a Cícero. Ele popularizou estilos de discurso, cunhou novas palavras (e limou da língua tantas outras), criou parâmetros de expressão em latim. De certa forma, pode-se dizer que Cícero está até os dias atuais impregnado no modo como pessoas públicas se expressam, por exemplo políticos e juristas. Obviamente Cícero não exerce tanto fascínio simplesmente porque foi Cícero, mas pela qualidade da sua expressão e seus estilos \cite{fishwick}.

\par É importante lembrar, entretanto, que Cícero não pode ser tomado como um autor que representa toda a grandeza do latim clássico sozinho. É necessário considerar isso por todo esse trabalho. O que será analisado pode muito bem nem mesmo representar toda a produção de Cícero de forma justa, vale dizer.

\par Será estudada uma das grandes características de Cícero, a chamada elegantia. Elegantia se traduz não simplesmente por elegância como seria de se esperar pela aparência de cognato, mas como um refinamento lexical e uma perfeita escolha de palavras para expressar uma idéia. Elegantia é saber utilizar a palavra certa, no momento certo, de maneira adequada \cite{palmer}. \citeonline{kennedy} também lembram elogios do próprio Cícero a César em seu De Analogia. No texto, Cícero diz que a escolha precisa de palavras ao falar e escrever é um dos vários requisitos para ser um cidadão romano. Portanto, nada mais adequado do que tentar enxergar como isso de dava utilizando a linguística de corpus.

\par A lista de textos a serem analisados ainda não está totalmente definida, mas os corpora serão construídos sempre que possível utilizando fontes abertas ou livres de restrição, a fim de que outros possam sem dificuldades utilizar os corpora gerados por esse trabalho. Embora já se tenha visto corpora de latim clássico (cobrindo alguns textos e discursos de Cícero, inclusive), pesquisadores e alunos somente tiraram conclusões de estilística deles e não desenvolveram pesquisa sobre os dados em si. Vale notar ainda que, segundo \citeonline{brill} a respeito dos famosos bancos de dados de latim do Packard Humanities Institute (\sigla{PHI}{Packard Humanities Institute})\footnote{O mesmo parece se aplicar aos corpora de latim clássico do projeto Perseus, da Tufts University}, \textit{citation of this resource has now become commonplace in any article touching upon Ciceronian word usage. Perhaps because the ease of retrieval of these data makes conclusions drawn from them less prized, publication of specifically computer-based work on Ciceronian vocabulary and usage is still in the future}.

\section{Stopwords}
\par O catálogo de stopwords é uma dependência natural dos corpora pois é ele que filtra resultados indesejados ou que poderiam eventualmente poluir os dados. Stopwords, do ponto de vista de um computador, nada mais são que palavras ou agrupamento de palavras que interrompem o processamento da linguagem e modificam esse processamento de acordo com alguma regra pré-estabelecida. Toda língua possui uma lista de stopwords para processamento, porém o latim ainda não conta com uma verdadeiramente utilizável.

\par Embora pudessem ter sua criação automatizada por algorítimos de textos, catálogos de stopwords geralmente são construídos manualmente por linguistas. Logo, não existe um catálogo definitivo ou verdadeiramente completo para uma determinada língua, somente mais abrangentes e menos abrangentes, a depender do uso que será dado a eles.

\par Em computação, stopwords não são utilizadas somente para filtrar resultados indesejados ou que não são o foco da pesquisa linguística, muitas vezes são termos repetitivos que levam a uma pior performance de um sistema e que se não forem ignorados diminuem a legibilidade dos resultados. Por exemplo, é bastante comum filtrar-se preposições, conjunções e pronomes, uma vez que eles pouco influenciam na maioria das análises computacionais de um texto e representam um conjunto de termos funcionais --- sem muita significância --- que podem não só poluir os dados do ponto de vista linguístico \cite{sardinha-wordsmith} mas também dificultar a análise computacional \cite{makrehchi}. Todavia, é claro que tais filtros podem ser desligados se tais termos funcionais da língua forem de fato o foco da pesquisa.

\par Para o latim será criado, sob orientação, um catálogo geral de stopwords (no modelo clássico, uma lista finita de termos), e catálogos menores onde se encontrará somente preposições, ou somente pronomes e assim por diante, para que os alunos e pesquisadores possam refinar melhor o uso de stopwords em latim.

\section{Python}
\par Tanto o NLTK quanto ferramentas auxiliares desse trabalho serão feitos utilizando a linguagem de programação Python. A escolha dessa linguagem de programação se dá, entre outros motivos, pelo fato do NLTK utilizá-la internamente para processar línguas naturais. Também, a facilidade de leitura dos código-fonte da linguagem por pessoas que não são da área de computação é um fator chave \cite{perkins}. Por Python ser uma linguagem de alto nível, ou seja, de alta abstração ao se programar com ela, o desenvolvimento de progras é mais rápido com ela e permite que ele seja menos críptico para terceiros que precisem analisá-lo.

\par A escolha de Python junto com o NLTK vai permitir uma autonomia muito grande para se criar, analisar e trabalhar de forma geral os corpora de latim clássico. Algumas alternativas para análise linguística de corpora\footnote{Há uma lista mais completa em http://linguistlist.org/sp/GetWRListings.cfm?WRAbbrev=Software} incluem os programas WordSmith Tools, Perseus Hopper (a versão web do projeto Perseus), Diogenes, Wmatrix, LanguageWare da IBM e LIWC. Infelizmente, quase todos esses programas possuem limitações tecnológicas, quando não de preço também. Utilizando a linguagem Python junto com o NLTK se tem uma possibilidade de desenvolvimento, customização e aprendizado muito grande. Digamos que se precise corrigir algum componente interno de um desses softwares, seja porque o erro é aparente ou porque o pesquisador suspeita do seu comportamento. Isso não é possível com eles, mas é com Python e o NLTK, tornando estes até mesmo mais cientificamente confiáveis.

\section{Análise de Frequência}
\par Será usado o método de análise de frequência estatística como forma básica de obter os resultados de vocabulários dos corpora. Análise de frequência é uma forma de, através de pequenos experimentos automáticos (utilizando-se programação e tentando-se obter combinações de um termo em um texto, por exemplo), descobrir a proporção de uso de uma palavra qualquer em relação ao todo de um corpus específico. Essa é uma das atividades primordiais da linguística de corpus, muitas vezes chamada simplesmente de estatística lexical por aderentes da linguística quantitativa.

\par Para chegar ao ponto de usar uma análise de frequência, é preciso antes passar pela montagem dos corpora, da criação de filtros e estipular outros parâmetros do processamento. Porém, uma vez que isso é feito, a análise de frequência se mostra relativamente simples e bastante proveitosa. É através da análise de frequência, por exemplo, que se pode tentar confirmar a Lei de Zipf para o latim clássico de Cícero, que postula que a frequência de uso de uma determinada palavra de uma língua qualquer é inversamente proporcional a sua posição em uma lista ordenada de palavras da língua. Em outras palavras, isso significa que um número baixo de termos da língua correspondem a maior porção do uso efetivo dela \cite{tesitelova}.

\chapter{Cronograma}
\par As tarefas necessárias para a conclusão do trabalho devem ser divididas em etapas sequenciais, visto que uma depende da outra. Muito provavelmente as etapas serão repassadas após uma primeira "rodada" para afinar os resultados e aparar problemas encontrados durante o caminho, antes da conclusão total da monografia.

\par 	As etapas e seus tempos de conclusão serão divididas como a seguir:

\section{Definição}
\par Definir o escopo do projeto e o que se pretende fazer. Especificar limites para não perder o foco dos objetivos nem se perder em superficialidade. Este projeto de monografia propriamente dito. Tempo estimado: Set-Out 2011. Complexidade: média.

\section{Coleta}
\par Agregação do material para montagem dos corpora, pensando já na relevância dos textos, utilidade deles e abrangência linguística. Será preciso orientação próxima para que o material coletado seja relevante e representativo. Tempo estimado: Out-Nov 2011. Complexidade: média.

\section{Montagem}
\par A construção efetiva dos corpora de acordo com os formatos esperados pelo projeto NLTK e uma categorização básica dos textos para facilitar análise futura. Nessa etapa serão disponibilizados os corpora do trabalho para que outros do setor e departamento também possam utilizá-lo. Tempo estimado: Nov-Dez 2011. Complexidade: baixa.

\section{Filtragem}
\par Especificação de certos filtros a serem utilizados em análises dos corpora (como a necessidade de se catalogar stopwords da língua, por exemplo). Essa etapa também precisará de orientação próxima, pois um conhecimento profundo do latim será necessário para que os dados não fiquem poluídos. Tempo estimado: Mar 2012. Complexidade: baixa.

\section{Análise}
\par Etapa final da análise de todo o material organizado, bem como obtenção de dados estatísticos da língua para uso futuro em sala de aula ou pesquisa acadêmica. Gerar um modelo de uso do material resultante desse trabalho. Tempo estimado: Abr-Jun 2012. Complexidade: alta.

% Existem ainda: abbrv, acm, alpha, amsalpha, amsplain
\bibliographystyle{abnt-alf}	
\bibliography{monografia}

%\apendice
%\chapter{Primeiro apêndice}
%\par Apêndices são textos elaborados pelo autor a fim de complementar sua argumentação.

%\anexo
%\chapter{Primeiro anexo}
%\par Anexos são documentos não elaborados pelo autor, que servem de fundamentação, comprovação ou ilustração.

\end{document}
